<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<title>Corrseg</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            color: #333;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
        }
        .container {
            width: 80%;
            margin: 0 auto;
            padding: 50px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
            max-width: 800px;
        }
        h1, h2 {
            color: #555;
        }
        .section {
            margin-bottom: 30px;
        }
        img {
            max-width: 80%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        iframe {
            width: 100%;
            height: 500px;
            border: 1px solid #ccc;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="section">
            <h1 style="text-align: center;">Corrseg</h1>
            <h2 style="text-align: center;">UNet-inspired CNN for Semantic Segmentation</h2>
        </div>

        <div class="section">
            <h3>Problem Description</h3>
            <p>This project was developed for a <a href="https://challengedata.ens.fr" target="_blank">ChallengeData</a> competition, presented by <a href="https://www.slb.com/" target="_blank">SLB</a> and focused on semantic segmentation. <a href="https://en.wikipedia.org/wiki/Image_segmentation" target="_blank">Semantic segmentation</a> is a crucial task in computer vision, involving the classification of each pixel in an image into a specific category. You can read more about the competition and the requirements <a href="https://challengedata.ens.fr/participants/challenges/144/" target="_blank">here</a>.</p>
        </div>

        <div class="section">
            <h3>Network Diagram</h3>
            <img src="./assets/images/my_arch.png" alt="Network Diagram">
        </div>

        <div class="section">
            <h2>How did I do it</h2>
            <p>The UNet-inspired Convolutional Neural Network architecture was chosen due to its effectiveness in biomedical image segmentation. The model includes an encoder-decoder structure that helps in capturing the context and fine details of the images.
                There are three triplets for down-sampling (blue) and two for upsamping. There is also one skip conncetion, which helps in preventing vanishing gradient.
            </p>
            <h3>Data Preprocessing</h3>
            <p>Before training the model, the input data needs to be preprocessed. First, I imputed <code>NaN</code> values with 0's. Next, I applied horizontal and vertical flip to augment the dataset. This is a convenient trick when dealing with CNNs, since it increases your training set by several factors and also helps the network generalize the desired patterns better. Treating outliers seemed to make the predictions worse.
            </ul>
            <h3>Network Architecture</h3>
            <p>The downsampling step involves three consecutive convolutional layers, each followed by a ReLU activation function. This helps in capturing the high-level features and reducing the spatial dimensions of the input.</p>
            <p>The upsampling step also consists of three convolutional layers, each followed by a ReLU activation function. This helps in recovering the spatial dimensions and generating the final segmentation map. I tried applying <a href="https://en.wikipedia.org/wiki/Batch_normalization" target="_blank">batch normalization</a>, but it degraded the perfomance. <a href="https://en.wikipedia.org/wiki/Dilution_(neural_networks)#Dropout" target="_blank">Droput</a> was also unhelpful, probably because the network was not that close to being overly complex given the input.</p>
            <p>The weights of the network are initialized using a Kiming He normal distribution. The Kaiming initialization method is calculated as a random number with a Gaussian probability distribution with a mean of 0.0 and a standard deviation of \( \sqrt{\frac{2}{n}} \), where \(n\) is the number of inputs to the node. This initialization strategy helps in stabilizing the training process and improving the convergence speed. It works particularly well for the task of semantic segmentation. You can read more about it in their paper <a href="https://arxiv.org/abs/1502.01852" target="_blank">here</a></p>
            <p>Unlike traditional UNets, this network does not use maxpooling layers due to the small dimensions of the input images. Instead, it relies on the convolutional layers to halve the dimensions while also learning features. You could say, we have too few pixels to lose. Moreover, the dimension being only \(36\times 36\), I could only easily make a "three level" network instead of a 5 level one. Once the dimension of the input into a hidden layer becomes odd, the things get a bit more complicated.</p>

            <h3>Training</h3>
            <div class="section">
                <div class="section"></div>
                <div style="display: flex; justify-content: center; align-items: center;">
                    <figure>
                        <img src="./assets/images/loss.png" alt="Loss Function" style="max-width: 100%; height: auto;">
                        <figcaption style="text-align: center;">Loss Function</figcaption>
                    </figure>
                    <figure>
                        <img src="./assets/images/jaccard.png" alt="Jaccard" style="max-width: 100%; height: auto;">
                        <figcaption style="text-align: center;">Estimated IoU</figcaption>
                    </figure>
                </div>
            </div>
            <p>The model was trained using the Adam optimizer with a learning rate of 0.0001. The loss function used was the binary cross-entropy loss, which is suitable for binary classification tasks like semantic segmentation. The model was trained for 84 epochs with a batch size of 128 for training and 256 for validation. Why different sizes? Training requires roughly twice as much memory (forward pass and backpropagation) than validation (only a forward pass). The training data was split into 80% training and 20% validation sets to monitor the model's performance during training. In the competition, the score is given as the intersection of the union or <a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank">Jaccard Index</a> over the image pixels. Hence, IoU was estimated at every iteration as the average over all batches of validation examples. </p>

        </div>

        <div class="section">
            <h3>Leaderboard</h3>
            <img src="./assets/images/leadboard.png" alt="Leaderboard">
            <p>If you would like to see my code, hit me up on LinkedIn and I will gladly share it.</p>
        </div>

        <div class="section">
            <h3>More Details</h3>
            <p>For more details, visit the <a href="https://challengedata.ens.fr/participants/challenges/144/" target="_blank">project page</a>.</p>
            <img src="./assets/images/challenge-data.png" alt="Details Screenshot">
        </div>
    </div>
</body>
</html>
